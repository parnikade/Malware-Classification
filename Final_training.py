import os
import pandas as pd
from sklearn.model_selection import train_test_split
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import LSTM
import matplotlib.pyplot as plt
from keras.layers import Embedding
from sklearn.svm import SVC
from sklearn.metrics import confusion_matrix, accuracy_score, \
     classification_report
from keras.models import load_model


label_dict = {
    'cycbot': 1, 'rbot': 2, 'vobfus': 3, 'zbot': 4, 'zeroaccess': 5
}
# list out keys and values separately
key_list = list(label_dict.keys())
val_list = list(label_dict.values())


def read_data(chunk_size=256, data_type='train', length=500):
    cd = '/Users/parnikade/Documents/Pota/SJSU/fall20/Coding'
    dir_type = 'train_entropy_'
    data = {}
    data['entropy'] = []
    if data_type == 'predict':
        dir_type = 'test_entropy_'
        data['file_name'] = []
    else:
        data['label'] = []
    wd = os.path.join(cd, dir_type + str(chunk_size))
    max_entropy_len = 0
    min_entropy_len = float('inf')
    for root, dirs, files in os.walk(wd):
        for file in files:
            file_path = os.path.join(root, file)
            label = file.split('_')[0]
            with open(file_path, "r") as fp:
                lines = fp.readlines()
                d = []
                for line in lines:
                    l = float(line.strip())
                    if l != 0:
                        d.append(l)
                if data_type == 'predict':
                    data['file_name'].append(file)
                else:
                    data['label'].append(label_dict.get(label))
                data['entropy'].append(d)
                if len(d) > max_entropy_len:
                    max_entropy_len = len(d)
                if len(d) < min_entropy_len:
                    min_entropy_len = len(d)
    i = 0
    while i < length:
        data[i] = []
        i += 1

    for d in data['entropy']:
        count = 0
        while count < length and count < len(d):
            data[count].append(d[count])
            count += 1
        while count < length:
            data[count].append(0.)
            count += 1

    del data['entropy']
    df = pd.DataFrame.from_dict(data=data)
    print(df)
    return df


def training_LSTM(df):
    X = df.drop(['label'], axis=1)
    y = df['label']

    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,
                                                        random_state=3)

    model = Sequential()
    model.add(Embedding(input_dim=3924, output_dim=500, input_length=500))
    model.add(LSTM(500))
    model.add(Dense(500, activation='softmax'))
    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy',
                  metrics=['accuracy'])
    model.summary()
    history_lstm = model.fit(X_train, y_train, epochs=40, batch_size=16,
                             validation_data=(X_test, y_test), shuffle=False)

    model.save('lstm500_model_short.h5')

    plt.plot(history_lstm.history['loss'], label='train')
    plt.plot(history_lstm.history['val_loss'], label='test')
    plt.legend()
    plt.show()

    plt.plot(history_lstm.history['accuracy'], label='train')
    plt.plot(history_lstm.history['val_accuracy'], label='test')
    plt.legend()
    plt.show()


def pedict_challenge_data_LSTM(df, chunk_size=256):
    data = df.drop(['file_name'], axis=1)
    file_name_df = df['file_name']
    model = load_model('lstm500_model_short.h5')
    yhat = model.predict_classes(data, verbose=0)
    #yhat = np.argmax(model.predict_classes(data), axis=-1)
    label = [key_list[val_list.index(y)] for y in yhat]
    label_df = pd.DataFrame(label, columns =['Label'])
    result = pd.concat([file_name_df, label_df], axis=1)
    result.to_excel(f'challenge_solution_LSTM_{chunk_size}.xlsx')
    print(result)


def training_SVM(df_train, df_challenge):

    X = df_train.drop(['label'], axis=1)
    y = df_train['label']

    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,
                                                        random_state=3)

    svclassifier = SVC(kernel='rbf', gamma='auto', verbose=True,
                       random_state=2)
    # selector = RFE(svclassifier, n_features_to_select=1300, step=1)
    svclassifier.fit(X_train, y_train)
    y_pred = svclassifier.predict(X_test)

    print(f'Confusion Matrix \n {confusion_matrix(y_test, y_pred)}')
    print("Accuracy with P-R scores and F-score")
    print(classification_report(y_test, y_pred))
    print(accuracy_score(y_test, y_pred))
    print()

    X_pred = data = df_challenge.drop(['file_name'], axis=1)
    file_name_df = df_challenge['file_name']
    svclassifier_pred = SVC(kernel='rbf', gamma='auto', verbose=True,
                       random_state=0)
    svclassifier.fit(X, y)
    y_pred_challenge = svclassifier.predict(X_pred)
    label = [key_list[val_list.index(y)] for y in y_pred_challenge]
    label_df = pd.DataFrame(label, columns =['Label'])
    result = pd.concat([file_name_df, label_df], axis=1)
    result.to_csv('challenge_solution_SVM_256.csv')
    print(result)
    print(y_pred_challenge)



def main():
    chunk_size = 256
    df_train = read_data(chunk_size, data_type='train', length=500)
    df_challenge = read_data(chunk_size, data_type='predict', length=500)
    training_LSTM(df_train)
    training_SVM(df_train, df_challenge)
    pedict_challenge_data_LSTM(df_challenge, chunk_size)


if __name__ == '__main__':
    main()
